{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a haiku about recursion in programming:\n",
      "\n",
      "Function calls itself  \n",
      "Diving deeper each time in  \n",
      "Until the base case\n"
     ]
    }
   ],
   "source": [
    "# import openai\n",
    "# import os\n",
    "# \n",
    "# # Read API key\n",
    "# api_key_file = os.path.expanduser(\"~/openai.key\")\n",
    "# with open(api_key_file, \"r\") as file:\n",
    "#     openai.api_key = file.read().strip()\n",
    "# \n",
    "# model = \"o1-preview-2024-09-12\"\n",
    "# messages =[{'role': \"user\", 'content': \"Write a haiku about recursion in programming.\"}]\n",
    "# response = openai.chat.completions.create(model=model, messages=messages)\n",
    "# \n",
    "# generated_text = response.choices[0].message.content\n",
    "# print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T20:01:06.498526Z",
     "start_time": "2024-12-11T20:00:56.148721Z"
    }
   },
   "id": "fdf95c734026acdf"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../supplemental_data/gpt_o1_preview_VIGNETTE/PMID_15673476.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 132\u001B[0m\n\u001B[1;32m    129\u001B[0m dry_run \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;66;03m# Fetch prompts and metadata\u001B[39;00m\n\u001B[0;32m--> 132\u001B[0m prompts_with_metadata \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_and_extract_prompts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdry_run\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;66;03m# Generate completions\u001B[39;00m\n\u001B[1;32m    135\u001B[0m prepend_instructions_and_generate(prompts_with_metadata, instruction, output_directory, model_name, dry_run)\n",
      "Cell \u001B[0;32mIn[2], line 48\u001B[0m, in \u001B[0;36mfetch_and_extract_prompts\u001B[0;34m(repo_url, output_dir, dry_run)\u001B[0m\n\u001B[1;32m     44\u001B[0m         prompts\u001B[38;5;241m.\u001B[39mappend((file_name, text_section))\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dry_run:\n\u001B[1;32m     47\u001B[0m             \u001B[38;5;66;03m# Save the prompt locally\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m             \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m     49\u001B[0m                 file\u001B[38;5;241m.\u001B[39mwrite(text_section)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mHTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PythonProject/malco/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    322\u001B[0m     )\n\u001B[0;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../supplemental_data/gpt_o1_preview_VIGNETTE/PMID_15673476.txt'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def fetch_and_extract_prompts(repo_url, output_dir, dry_run=False):\n",
    "    \"\"\"\n",
    "    Fetches prompt files from the GitHub repository and extracts the [text] section.\n",
    "\n",
    "    Args:\n",
    "        repo_url (str): GitHub raw directory URL where the prompts are located.\n",
    "        output_dir (str): Directory to save the extracted prompts.\n",
    "        dry_run (bool): If True, skips writing files to the output directory.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted prompts with metadata.\n",
    "    \"\"\"\n",
    "    # Fetch the HTML for the repository directory\n",
    "    response = requests.get(repo_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse file list and construct absolute GitHub URLs\n",
    "    file_urls = re.findall(r'href=\"(/[^\"]+\\.txt)\"', response.text)\n",
    "    file_urls = [f\"https://github.com{url}\" for url in file_urls if \"cases\" in url]\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    for file_url in file_urls:\n",
    "        # Convert to raw GitHub URL\n",
    "        full_url = file_url.replace(\"github.com/\", \"raw.githubusercontent.com/\").replace(\"/blob/\", \"/\")\n",
    "        file_name = os.path.basename(file_url)\n",
    "\n",
    "        try:\n",
    "            # Fetch the file content\n",
    "            file_response = requests.get(full_url)\n",
    "            file_response.raise_for_status()\n",
    "            content = file_response.text\n",
    "\n",
    "            # Extract the [text] section\n",
    "            match = re.search(r\"\\[text\\](.*?)$\", content, re.DOTALL)\n",
    "            if match:\n",
    "                text_section = match.group(1).strip()\n",
    "                prompts.append((file_name, text_section))\n",
    "\n",
    "                if not dry_run:\n",
    "                    # Save the prompt locally\n",
    "                    with open(os.path.join(output_dir, file_name), \"w\") as file:\n",
    "                        file.write(text_section)\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Failed to fetch {full_url}: {e}\")\n",
    "\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def prepend_instructions_and_generate(prompts_with_metadata, instruction, output_dir, model, dry_run=False):\n",
    "    \"\"\"\n",
    "    Prepends instructions to prompts, optionally calls OpenAI API for completion,\n",
    "    and emits a result file with service answers and metadata.\n",
    "\n",
    "    Args:\n",
    "        prompts_with_metadata (list): List of tuples (file_name, prompt_text).\n",
    "        instruction (str): Instruction to prepend to each prompt.\n",
    "        output_dir (str): Directory to save prompt files and results.\n",
    "        model (str): OpenAI model name to use.\n",
    "        dry_run (bool): If True, skips calling the API and writing result files.\n",
    "    \"\"\"\n",
    "    if not dry_run:\n",
    "        # Read API key\n",
    "        api_key_file = os.path.expanduser(\"~/openai.key\")\n",
    "        with open(api_key_file, \"r\") as file:\n",
    "            openai.api_key = file.read().strip()\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    result_rows = []  # To store rows for the result file\n",
    "\n",
    "    for file_name, prompt in prompts_with_metadata:\n",
    "        # Prepend instructions\n",
    "        full_prompt = f\"{instruction}\\n\\n{prompt}\"\n",
    "\n",
    "        if dry_run:\n",
    "            # Print simulated workflow for dry run\n",
    "            print(f\"Simulated API call for file: {file_name}\")\n",
    "            print(f\"Prompt:\\n{full_prompt}\\n\")\n",
    "            result_rows.append([\"Simulated response\", file_name])\n",
    "        else:\n",
    "            # Call OpenAI API\n",
    "            messages = [{'role': \"user\", 'content': full_prompt}]\n",
    "            response = openai.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "            # Extract generated text\n",
    "            generated_text = response.choices[0].message.content\n",
    "            result_rows.append([generated_text, file_name])\n",
    "\n",
    "    if not dry_run:\n",
    "        # Emit result file\n",
    "        result_file_path = os.path.join(output_dir, \"result.csv\")\n",
    "        with open(result_file_path, \"w\", newline=\"\") as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow([\"service_answer\", \"metadata\"])  # Header row\n",
    "            csv_writer.writerows(result_rows)\n",
    "\n",
    "        print(f\"Processing completed. Results saved to {result_file_path}\")\n",
    "    else:\n",
    "        print(\"Dry run completed. No files written or API calls made.\")\n",
    "\n",
    "\n",
    "repo_url = \"https://github.com/monarch-initiative/phenopacket2prompt/blob/main/docs/cases\"\n",
    "output_directory = \"../supplemental_data/gpt_o1_preview_VIGNETTE/\"\n",
    "\n",
    "# make directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "instruction = (\n",
    "    \"I am running an experiment on a clinical case report to see how your diagnoses compare with those of human experts.\\n\"\n",
    "    \"I am going to give you part of a medical case. In this case, you are “Dr. GPT-4”, an AI language model who is providing\\n\"\n",
    "    \"a diagnosis. Here are some guidelines. First, there is a single definitive diagnosis, and it is a diagnosis that is known\\n\"\n",
    "    \"today to exist in humans. The diagnosis is almost always confirmed by some sort of genetic test, though in rare cases\\n\"\n",
    "    \"when such a test does not exist for a diagnosis the diagnosis can instead be made using validated clinical criteria or\\n\"\n",
    "    \"very rarely just confirmed by expert opinion. After you read the case, I want you to give a differential diagnosis with\\n\"\n",
    "    \"a list of candidate diagnoses ranked by probability starting with the most likely candidate. Each candidate should be\\n\"\n",
    "    \"specified with disease name. For instance, if the first candidate is Branchiooculofacial syndrome and the second is\\n\"\n",
    "    \"Cystic fibrosis, provide this:\\n\\n\"\n",
    "    \"1. Branchiooculofacial syndrome\\n\"\n",
    "    \"2. Cystic fibrosis\\n\\n\"\n",
    "    \"This list should provide as many diagnoses as you think are reasonable. You do not need to explain your reasoning,\\n\"\n",
    "    \"just list the diagnoses. Here is the case:\"\n",
    ")\n",
    "\n",
    "model_name = \"o1-preview-2024-09-12\"\n",
    "dry_run = False\n",
    "\n",
    "# Fetch prompts and metadata\n",
    "prompts_with_metadata = fetch_and_extract_prompts(repo_url, output_directory, dry_run)\n",
    "\n",
    "# Generate completions\n",
    "prepend_instructions_and_generate(prompts_with_metadata, instruction, output_directory, model_name, dry_run)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T16:51:14.357471Z",
     "start_time": "2024-12-12T16:51:13.543866Z"
    }
   },
   "id": "8d600bc16a729e6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a26c89ae93bfbdc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
